Gist DL 2 Transformer
----

* Transformer is a Encoder-Decoder Archi

* Bert & GPT are simplified or kinda concept arrivied from Transformers

  GPT :- Decoded Only Archi
  BERT :- Encoded only Archi


-> Revolution of Models (Time Series)
   ARIMA -> LSTM -> Transformer -> BERT

[MASKING]
* Masking is performed so that even if data is present in timeline & if we want to ignore
  Some data from that as per future prospect We can use Masking
