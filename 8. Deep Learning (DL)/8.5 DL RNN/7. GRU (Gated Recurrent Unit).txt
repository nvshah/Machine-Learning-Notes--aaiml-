7. GRU ( Gated Recurrent Unit)
-----------------------------

ref :
[GRU slide - Cristopher Ola]
https://www.slideshare.net/hytae/recent-progress-in-rnn-and-nlp-63762080
[Blog]
https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be

- simplified version & inspired from LSTM
- faster to train

- Faster to compute backPropogation
  |
  fewer derivative to worry about

- Only 2 inputs in case of GRU (ie Cell State is managed internally)
   - Output & Curr-Inp

- Cell State is taken care in Outputs & Inputs

- 2 gates :- reset & update
    \
     Reset Structure
     Update Structure

- There are fewer eqns so faster to compute (derivative & backpropogation)
