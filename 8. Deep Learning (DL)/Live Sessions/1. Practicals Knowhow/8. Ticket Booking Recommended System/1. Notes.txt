ref : https://drive.google.com/file/d/1xwCWqsPhvMa4aYsZqprAlOeKqHTVQA2z/view

1. Notes 
---------

-> You can take output of any layers from Fully Connected layers as an Embedding vectors

=> Ensembling is a technique that is to be used when the comprising models differ by some extent

_______

=> At its very core Matrix Factorization is multiplying user vector with item vector
-> All the embeddings are generated by Deep NN
-------------

- Each Trip is a Session


* Session based RS :
  ---
  - recommend trip based on trip-context

* Design Features at 3 levels
  - Trip, Location(Geographical), User


* Ensembles Models :
  ---
  - Train different models & combine them to generate result

* Models
  1. MLP
  2. GRU
  3. XlNet

* SMF :
  ---
  -> Session based Matrix Facotorization
  -> Dot product of Trip Vector & City Vector


1) MLP-SMF :
  ---
   -> City Embedding vector are also learnt during training of Deep NN network
   => Sequence information of cities are preserved via position

=> PRELU ( Parametric Relu)
    -> When you dont want dead cells

  Matrix Factorization Head :
  ----
  - Most imp unit for entire MLP-SMF unit

2) GRU-SMF :
  ---
  - GRU is RNN
    & it is used in compare to LSTM as there are less param to be learned

  -> Here we are using GRU to encode the Sequence information

* MS-SMF : (Multi Stage Matrix Factorization)
  -> It is often used when there are lots of items
     &
     item in current case is cities


3) XLNet-SMF :
  ----
  -> XlNet is Transformer/Bert like model
  -> Categorical feature are converted into embeddings
  -> Continuous features are inputted as it is

  city -> Embedded Vec -> Goes through some layer & then -> Sequence of Embeddings(Cities)


Q) XLNet vs Bert :
-------------
  In Bert : We see both that come before & come after
  but
  In our case we only want to see the things (cities) that come before (ie Forward Prediction)
  &
  this is supported by XlNet

  In case of BERT, it would be leakage (as it can see data ahead as well)


* Ablation Study :
  ----
  - Does adding something is useful (ie improve things)


* RELU vs Leaky Relu vs PRELU ??
 -> Leaky Relu the slope in -ve side is constant
    whereas
    in PRELU it is parameterized
      \
       -> RELU's slope on -ve side is also learned via Back-Propogation


* Data Augmentation :
  ----
  - To avoid overfitting (as due to less trips data available)
    |
    reverse trip data generate (ie x -> y -> z := z -> y -> x)
