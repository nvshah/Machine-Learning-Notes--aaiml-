Notes.txt

ref : https://colab.research.google.com/drive/1iyOE_5MZAfRGv427jr6EjWwdsLVd2OJ3

- huggingface library

---------

=> The inbuild models from transformer (such as DistilBertTokenizer, etc...)
   are pre_trained from large amount of data from google, wikipedia, etc...

-> The output corresp to [cls] are the sentence vectors
   The output corresp to any words are the word vectors

=> At the end of daya Your BERT is only a bunch of encoders stacked together

---------

1) BERT - Featurization

* Tokenization

* Sub-Tokenization
  ------
  -> smart tokenization system that recognize the diff form (ie adj, adverb, etc...) of same words
     Eg fast, faster

  Character Based Models

  -> Not every Transformer based model utilises the SubTokenization, but some of them do

* Different Techniques in BERT :
  ----
  1. BERT
  2. RoBERT
  3. DistilBERT
  4. XLNet

  => By default people uses BERT or DistilBERT as at run time its better


* PreTrained Model :
  ---
  Eg DistilBertTokenizer.from_pretrained()

  -> Consider weights from pre-trained model (done by library itself)

  -? You just need to know the name of pretrained model

* BERT
-------

=> The output from BERT for [cls] is very good representation of entire sentence (ie multiple words)
   we have

-> BERT basically is good for 2 things
   1. Feture Extraction
   2. Fine Tuning

- Batch is represented by None

Process :-
  - Sentence is breaken down into tokens
  - then this tokens are passed alongwith bacth num as inp to model

-> We can even get the hidden layers vectors from output
   |
   Thus we can extract features both from last layer & hidden layers as well

