Doubts DL

-----
1) Internal Covariance Shift & Batch Normalization :
   ----
    -> ref : Deep NN (Videos)

   Q) : Does Internal Cov Shift & Batch Norm are concerned only with Mini-Batch SGD for Deep NN
        or 
        Thenn what abt Single-Poit GD ??


------
2) Batch Normalization | Input Distb & Larger Learning Rate :
  -----
   -> How Does Similar Distribution among Batch in Mini Batch SGD, helps us to 
      have larger Learning Rate in Update Eqn (esp in case of Batch Normalization Process) ??

   Que Ref :- DL ->  Deep Multi-Layer Perceptron -> 2.5 Batch Normalization



------
Q3) Derivative of Max-Pooling

    What if some cell value changes more rapidly than other adjacent cell in same grp
    then do we need to change derivative update Logic in-between (ie 1 or 0) in case of maxPooling ??

    Que Ref Video :- 
       Sec 4.7 CNN training: Optimization, (time: 6:46 PM)

    Consider Eg :-
       \
         1  3                                            0  0
         2  4    =>  Max-Pooling -> 4  So Derivative ->  0  1

         Now if same above cell after sometime becomes

         2  5
         4  4    => then Max-Pooling -> 5 
                    Then according to Derivative Logic (Custom) => 0  0 
                                                                   0  1
    Ans :-
    ---
    ref : https://towardsdatascience.com/forward-and-backward-propagation-of-pooling-layers-in-convolutional-neural-networks-11e36d169bec