12. Bias-Varaince TradeOff

* Bias - Variance TradeOff :
  ---------------------

   -> The mathemetical basis for Underftirring & Overfitting is bias-variance tradeoff
   -< Imp theory of ML that underpins a lot concepts like underfitting & Overfitting

   -< It's much intutive to understand using Linear Regression Concept

   * Generalization Error : (Bias)^2 + Variance + Irreducible Error
        \
         It can be decomposed into 3 terms 1) Bias
          								   2) Variance 
          								   3) Irreducible Error

     Goal -> low generalization error

     * Irreducable Error :
       ---
        - error that you cannot further reduce (i.e stagnant )

     * Bias Error :
       ---
        - Errors due to simplyfing assumptions 
        - high-bias => Underfitting

        - geometrically :
             -> Let say our model assumes that Straight Line will be a decision surface or seperator
                i.e straigh line to seperate +ve & -ve pts
                
                But Curve - makes more sense than straight line actually

                But though a Curve would better define model, We simplify model usign straight line
                (not caring abt some missclassification)

                I am limiting myself to straigh line only

                This Simplification or assumption is known as High-Bias Model or Underfitting

             -> An error that happens due to simplifying assumption are known as Bias-Error

        - Logically Example :

        	Very Simplifying Assumption :
              -> Dominant class becomes class label of every query point 

            Eg In KNN, when k = n ; 80% +ve & 20% -ve
                Just predict +ve every time   // this is very simplifying assumption

            -> Such scenario is called high-biased error

    * Variance Error :
      --- 
       - variance : how much model changes as training data changes 
       - model is nothing but Decision Surface so Model change means change in decision surface

       - If model varys much with training data
       - High-Variance = Overfit

       -> When we perform random sampling every time for test-train split, pts like outlier can go 
          in diff class label randomly each time

          & If such outlier random belonging to diff labels randomly causes decision surface to change by great impact then it's called High-Variance Error in Model ie OverFitting

       So If 1) Train (70%) + Test (30%)
             2) Train (70%) + Test (30%)

       => So small changes in Training set results in very diff models(= decision surface) i,e large 
          changes in model
          Such a model is called High-Variance Model

          As K incr, this variance will reduces & such Variance error also reduces


    Thus reducn of Bias Err     = no underfit
         reducn of Variance Err = no Overfit


  * KNN & Bias-Variance :
   ------------------
    As K incr, Bias incr slightly & 
               Varince decr drastically

    Thus as bias affected slightly so Bias term is squared in decomposition of generalization err

    As K -> n, Bias become very High
               Variance become very low

    TradeOff is all abt balancing between 2
      \
       Good soln lies between 2 extremes i.e Overfit & Underfit
       As We change K balance is shifting


   => Thus all in all Considering Generalization Err & Bias-Var tradeoff
   			- First we try to reduce Variance 
   			- & then we go for reducing Bias

   			As Variance has slight upper edge of impact when compare to Bias As dimen incr


* Intutive Understanding of Bias-Variance :
  -------------------------------------
   - understanding in terms of an errors
   - using Train Error & Test Error

   1) Underfitting 

       -> train err high => high-biased => underfit
                    low  => low-biased 

   2) Overfitting

       1 train err low & test err high => high-variance => overfit

       2. train data changes slightly -> model change => overfit => high-variance
              \
               So in KNN when k=1, i.e even if 1 point changes in dataset
                         whole model changes
                         & thus high-variance problem i.e Overfitting 	