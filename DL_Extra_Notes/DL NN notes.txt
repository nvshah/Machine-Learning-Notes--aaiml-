DL NN notes

=> DL can automatically engineer fetaures for us

* Importance of NN or Multi Layered Structure :
   - Multi Layered Structure is most useful & imp way of addressign complex functions

=> For Back-Propogation Algo, Your Activation Function needs to be Differentiable.

[*IMP*]
=> You can think MLP as Multi layer generalization of Linear Regression or Logistic Regression 


* DropOut for Regularization

* Data Augmentation is esp used for Image Dataset 
   \
    used mostly in CNN

* Padding primarily meant for Batch Update, in LSTM (thus it can speed up update process)

* Skip Connection

* Dropout & Conv Layer & Fully Connected Layer :
  -------
  -> compare to 2 Conv layer, chances of dropout being present between 2 fully connected layers is more
     because
     there are more weights variable between 2 FC-layers, comparatively
     So need some sort of Regularization

* Auto Encoder & Word2Vec :
  ----
  - Auto Encoder helps you to understand how Word2Vec works internally (alikeness)

* Convert to Vector :
  --------
  - For Images (CNN) :- VGG16, ResNet
  - For NLP (text) :- BERT (Transformer)

=> CNN has become so good today because of the facility available to get vectors from Images
   (ie VGG16, ResNet)


* Inductive Biases :
  ----
  -> Prior-Info we consider to build or prepare our model

  => So lot of Deep Learning we learns have the inductive biases
     - CNN - via Convolution
       Text Info - via Seq Information
       Transformer - via Self Attention Mechanism
