3. ML- Models

* ML- modes on bytes files only :
  -----------------

  bytes files :- Featurization :- ( BOW + Hexadecimal conversion )

  => We will compare models basically by 2 KPI :- { Log-Loss & Miss-classification }

  Random Model :-
  ---
   -> 1) generate random probabilities for all classes ie Pij
      2) Normalize the values (inorder to make probabilities sum to 1)

  Confusion Matrix :
  ---

  Precision Matrix :
  ---
   - Column sum up to your 1 
     &
     each cell val denotes the precision val for actual class of your prediction

   => Ideal Precisionn Matrix :- Diagonal to be 1

  Recall Matrix :
  ----
   - Row Sum up to 1
   - cell-val recall value
    => Ideal Recall Matrix :- Diagonal to be 1


1) kNN Model Algo :
   -------------

  - CalibratedClassifier :- to get better probabilities value/output

  Cross Validation :-
   ---
    - For each {alpha} vs {Error-Measure}

   chance of overfitting is there as k=1 is chosen at moment in CV

2) Logistic Regression :
   ------------

3) Random Forest :
   -------
    hyper-Parameter Tuning :- only on #trees ie number of estimators = n_estimators

    Random Forest typically avoides overfitting because number of Trees is large


4) XGBoost :  
   ------
    hyper-Parameter Tuning :- only on #trees ie number of estimators = n_estimators


* Comparision of all 5 diff models :
  ---------------------------
   - based on Log-Loss & misclassification

     - XGBoost came out to be very optimal
     - at second Random Forest stood up
     - & afterwards KNN (but chance of Overfitting are also there in KNN)


_________________________________




